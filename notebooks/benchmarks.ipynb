{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generaitve AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our [Made With ML course](https://madewithml.com/) we will be fine-tuning an LLM for a supervised classification task. The specific class of LLMs we'll be using is called [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)). Bert models are encoder-only models and are the gold-standard for supervised NLP tasks. However, you may be wondering how do all the (much larger) LLM, created for generative applications, fare ([GPT 4](https://openai.com/research/gpt-4), [Falcon 40B](https://huggingface.co/tiiuae/falcon-40b), [Llama 2](https://ai.meta.com/llama/), etc.)?\n",
    "\n",
    "> We chose the smaller BERT model for our course because it's easier to train and fine-tune. However, the workflow for fine-tuning the larger LLMs are quite similar as well. They do require much more compute but Ray abstracts away the scaling complexities involved with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -qdm (c:\\users\\agusa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\agusa\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ygments (c:\\users\\agusa\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\agusa\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 438, in _error_catcher\n",
      "    yield\n",
      "  File \"C:\\Users\\agusa\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 561, in read\n",
      "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
      "  File \"C:\\Users\\agusa\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 527, in _fp_read\n",
      "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
      "  File \"C:\\Users\\agusa\\anaconda3\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 98, in read\n",
      "    data: bytes = self.__fp.read(amt)\n",
      "  File \"C:\\Users\\agusa\\anaconda3\\lib\\http\\client.py\", line 459, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"C:\\Users\\agusa\\anaconda3\\lib\\http\\client.py\", line 503, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"C:\\Users\\agusa\\anaconda3\\lib\\socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\agusa\\anaconda3\\lib\\ssl.py\", line 1274, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"C:\\Users\\agusa\\anaconda3\\lib\\ssl.py\", line 1132, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\agusa\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 180, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "  File \"C:\\Users\\agusa\\anaconda3\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 245, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"C:\\Users\\agusa\\anaconda3\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 377, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"C:\\Users\\agusa\\anaconda3\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 179, in resolve\n",
      "    self.factory.preparer.prepare_linked_requirements_more(reqs)\n",
      "  File \"C:\\Users\\agusa\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 552, in prepare_linked_requirements_more\n",
      "    self._complete_partial_requirements(\n",
      "  File \"C:\\Users\\agusa\\anaconda3\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 467, in _complete_partial_requirements\n",
      "    for link, (filepath, _) in batch_download:\n",
      "  File \"C:\\Users\\agusa\\anaconda3\\lib\\site-packages\\pip\\_internal\\network\\download.py\", line 183, in __call__\n",
      "    for chunk in chunks:\n",
      "  File \"C:\\Users\\agusa\\anaconda3\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 63, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"C:\\Users\\agusa\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 622, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"C:\\Users\\agusa\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 587, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"C:\\Users\\agusa\\anaconda3\\lib\\contextlib.py\", line 131, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"C:\\Users\\agusa\\anaconda3\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 443, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==0.27.8 tqdm==4.65.0 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to first sign up for an [OpenAI account](https://platform.openai.com/signup) and then grab your API key from [here](https://platform.openai.com/account/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[0;32m      2\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYOUR_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "openai.api_key = \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_on</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>2020-02-20 06:43:18</td>\n",
       "      <td>Comparison between YOLO and RCNN on real world...</td>\n",
       "      <td>Bringing theory to experiment is cool. We can ...</td>\n",
       "      <td>computer-vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>2020-02-20 06:47:21</td>\n",
       "      <td>Show, Infer &amp; Tell: Contextual Inference for C...</td>\n",
       "      <td>The beauty of the work lies in the way it arch...</td>\n",
       "      <td>computer-vision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>2020-02-24 16:24:45</td>\n",
       "      <td>Awesome Graph Classification</td>\n",
       "      <td>A collection of important graph embedding, cla...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>2020-02-28 23:55:26</td>\n",
       "      <td>Awesome Monte Carlo Tree Search</td>\n",
       "      <td>A curated list of Monte Carlo tree search pape...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>2020-03-07 23:04:31</td>\n",
       "      <td>AttentionWalk</td>\n",
       "      <td>A PyTorch Implementation of \"Watch Your Step: ...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id           created_on                                              title  \\\n",
       "0   6  2020-02-20 06:43:18  Comparison between YOLO and RCNN on real world...   \n",
       "1   7  2020-02-20 06:47:21  Show, Infer & Tell: Contextual Inference for C...   \n",
       "2   9  2020-02-24 16:24:45                       Awesome Graph Classification   \n",
       "3  15  2020-02-28 23:55:26                    Awesome Monte Carlo Tree Search   \n",
       "4  25  2020-03-07 23:04:31                                      AttentionWalk   \n",
       "\n",
       "                                         description              tag  \n",
       "0  Bringing theory to experiment is cool. We can ...  computer-vision  \n",
       "1  The beauty of the work lies in the way it arch...  computer-vision  \n",
       "2  A collection of important graph embedding, cla...            other  \n",
       "3  A curated list of Monte Carlo tree search pape...            other  \n",
       "4  A PyTorch Implementation of \"Watch Your Step: ...            other  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load training data\n",
    "DATASET_LOC = \"https://raw.githubusercontent.com/agusabdulrahman/Made-With-ML/main/datasets/dataset.csv\"\n",
    "train_df = pd.read_csv(DATASET_LOC)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['computer-vision', 'other', 'natural-language-processing', 'mlops']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique labels\n",
    "tags = train_df.tag.unique().tolist()\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inference dataset\n",
    "HOLDOUT_LOC = \"https://raw.githubusercontent.com/agusabdulrahman/Made-With-ML/main/datasets/holdout.csv\"\n",
    "test_df = pd.read_csv(HOLDOUT_LOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilitis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a few utility functions to make the OpenAI request and to store our predictions. While we could perform batch prediction by loading samples until the context length is reached, we'll just perform one at a time since it's not too many data points and we can have fully deterministic behavior (if you insert new data, etc.). We'll also added some reliability in case we overload the endpoints with too many request at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme()\n",
    "from sklearn.metrics import precisian_recall_fscore_support\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m assistant_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# assistant content (context)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m user_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhow are you\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# user content (message)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241m.\u001b[39mChatCompletion\u001b[38;5;241m.\u001b[39mCreate(\n\u001b[0;32m      7\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-0613\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      9\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_content},\n\u001b[0;32m     10\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: assistant_content},\n\u001b[0;32m     11\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_content},\n\u001b[0;32m     12\u001b[0m     ],\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mto_dict()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_todict()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'openai' is not defined"
     ]
    }
   ],
   "source": [
    "# Query Openai endpoint\n",
    "# Query OpenAI endpoint\n",
    "system_content = \"you only answer in rhymes\"  # system content (behavior)\n",
    "assistant_content = \"\"  # assistant content (context)\n",
    "user_content = \"how are you\"  # user content (message)\n",
    "response = openai.ChatCompletion.Create(\n",
    "    model=\"gpt-3.5-turbo-0613\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_content},\n",
    "        {\"role\": \"user\", \"content\": user_content},\n",
    "    ],\n",
    ")\n",
    "print(response.to_dict()[\"choices\"][0].to_todict()[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
